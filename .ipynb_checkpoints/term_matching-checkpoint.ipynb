{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q7PvvgOxNyZt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from functools import partial\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "#from IPython.html import widgets\n",
    "from collections import namedtuple\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import downloader as g_downloader\n",
    "import itertools\n",
    "\n",
    "# gensim uses logging, so set it up \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ctsu83FoNyZu"
   },
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    folder_path = os.environ.get(\"IR1_DATA_PATH\")\n",
    "    if not folder_path:\n",
    "        folder_path = \"./datasets/\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_location = os.path.join(folder_path, \"cacm.zip\")\n",
    "    \n",
    "    # download file if it doesn't exist\n",
    "    if not os.path.exists(file_location):\n",
    "        \n",
    "        url = \"https://surfdrive.surf.nl/files/index.php/s/M0FGJpX2p8wDwxR/download\"\n",
    "\n",
    "        with open(file_location, \"wb\") as handle:\n",
    "            print(f\"Downloading file from {url} to {file_location}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                handle.write(data)\n",
    "            print(\"Finished downloading file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(folder_path, \"train.txt\")):\n",
    "        \n",
    "        # unzip file\n",
    "        with zipfile.ZipFile(file_location, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TPLms_FqNyZw"
   },
   "outputs": [],
   "source": [
    "def read_cacm_docs(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM documents. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Returns: A list of 2-tuples: (doc_id, document), where 'document' is a single string created by \n",
    "            appending the title and abstract (separated by a \"\\n\"). \n",
    "            In case the record doesn't have an abstract, the document is composed only by the title\n",
    "    \"\"\"\n",
    "\n",
    "    file_dir = root_folder + \"cacm.all\"\n",
    "    keeping = False\n",
    "    temp = \"\"\n",
    "    doc_list =[]\n",
    "\n",
    "    with open(file_dir) as cacm_file:\n",
    "        for line in cacm_file:\n",
    "            line_begin = line[0:2]\n",
    "\n",
    "            if line_begin == \".I\":\n",
    "                doc_index = line.split(\" \")[1].replace(\"\\n\",\"\")\n",
    "                temp = \"\"\n",
    "\n",
    "            elif line_begin == \".T\":\n",
    "                keeping = True\n",
    "\n",
    "            elif line_begin == \".W\":\n",
    "                keeping = True\n",
    "                temp += \"\\n \"\n",
    "\n",
    "            elif line_begin in ['.B', '.A', '.N', '.K']:\n",
    "                keeping = False\n",
    "\n",
    "            elif (line_begin not in ['.I', '.T', '.W', '.B', '.A', '.N', '.X', '.K']):\n",
    "                if keeping:\n",
    "                    temp += line\n",
    "            else:\n",
    "                doc_list.append((doc_index, temp))\n",
    "                keeping = False\n",
    "\n",
    "    return doc_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HehAkHVFNyZw"
   },
   "outputs": [],
   "source": [
    "def read_queries(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM queries. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Returns: A list of 2-tuples: (query_id, query)\n",
    "    \"\"\"\n",
    "    \n",
    "    file_dir = root_folder + \"query.text\"\n",
    "    keeping = False\n",
    "    temp = \"\"\n",
    "    query_list =[]\n",
    "\n",
    "    with open(file_dir, \"r\") as query_file:\n",
    "        for line in query_file:\n",
    "            line_begin = line[0:2]\n",
    "\n",
    "            if line_begin == \".I\":\n",
    "                doc_index = line.split(\" \")[1].replace(\"\\n\",\"\")\n",
    "                temp = \"\"\n",
    "\n",
    "            elif line_begin == \".W\":\n",
    "                keeping = True   \n",
    "\n",
    "            elif line_begin == \".A\":\n",
    "                keeping = False\n",
    "\n",
    "            elif (line_begin not in ['.I', '.T', '.W', '.B', '.A', '.N', '.X', '.K']):\n",
    "                if keeping:\n",
    "                    temp += line\n",
    "            else:\n",
    "                query_list.append((doc_index, temp))\n",
    "                keeping = False\n",
    "                \n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NmTFHQ0nNyZy"
   },
   "outputs": [],
   "source": [
    "def load_stopwords(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Loads the stopwords. The dataset is assumed to be in the folder \"./datasets/\" by default\n",
    "        Output: A set of stopwords\n",
    "    \"\"\"\n",
    "    file_dir = root_folder + \"common_words\"\n",
    "\n",
    "    with open(file_dir , 'r') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "        \n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P6D4UbG6NyZy"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenizes the input text. Use the WordPunctTokenizer\n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    tokenized = nltk.WordPunctTokenizer().tokenize(text)\n",
    "    return tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aDdJ3Y57NyZz"
   },
   "outputs": [],
   "source": [
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stems the given token using the PorterStemmer from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    ps = nltk.stem.PorterStemmer() \n",
    "    return ps.stem(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PyajR9VuNyZz"
   },
   "outputs": [],
   "source": [
    "def process_text(text, stem=False, remove_stopwords=False, lowercase_text=False):\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if remove_stopwords and token.lower() in stopwords:\n",
    "            continue\n",
    "        if stem:\n",
    "            token = stem_token(token)\n",
    "        if lowercase_text:\n",
    "            token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YeRr6RUMNyZ0"
   },
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize \n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "} \n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text, stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True, \n",
    "} \n",
    "\n",
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, **config_2)))\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XtqI2gQaNyZ0"
   },
   "outputs": [],
   "source": [
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "        Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "        and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "        Input: a list of documents - (doc_id, tokens) \n",
    "        Output: An inverted index. [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    tf_index = {}\n",
    "\n",
    "    for doc in documents:\n",
    "        for token in np.unique(doc[1]):\n",
    "            doc_list = (doc[0], doc[1].count(token))\n",
    "\n",
    "            if token in tf_index.keys():\n",
    "                tf_index[token].append(doc_list)\n",
    "            else:\n",
    "                tf_index[token] = [doc_list]\n",
    "    \n",
    "    return tf_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Fps12HX9NyZ0"
   },
   "outputs": [],
   "source": [
    "#### Indexed documents based on the two configs\n",
    "\n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n",
    "\n",
    "# This function returns the tf_index of the corresponding config\n",
    "def get_index(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: tf_index_1,\n",
    "        2: tf_index_2\n",
    "    }[index_set]\n",
    "\n",
    "####\n",
    "#### Preprocessed query based on the two configs\n",
    "\n",
    "# This function preprocesses the text given the index set, according to the specified config\n",
    "def preprocess_query(text, index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        return process_text(text, **config_1)\n",
    "    elif index_set == 2:\n",
    "        return process_text(text, **config_2)\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "i6o1qWVENyZ1"
   },
   "outputs": [],
   "source": [
    "def bow_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query. \n",
    "        Note: You have to use the `get_index` function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    index = get_index(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    bag_dict = {}\n",
    "    for q in processed_query:\n",
    "\n",
    "        if q not in index:\n",
    "            continue \n",
    "\n",
    "        for doc_id, tf in index[q]:        \n",
    "            if doc_id not in bag_dict:\n",
    "                    bag_dict[doc_id] = 0.0\n",
    "            \n",
    "            bag_dict[doc_id] += 1.0 \n",
    "\n",
    "    sorted_result = sorted(bag_dict.items(), key=lambda tup: tup[1], reverse = True)\n",
    "\n",
    "    return sorted_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "42e_gaIFNyZ2"
   },
   "outputs": [],
   "source": [
    "def compute_df(documents):\n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency)\n",
    "    \"\"\"\n",
    "    doc_freq = {}\n",
    "    \n",
    "    for i in range(len(documents)):\n",
    "        tokens = documents[i]\n",
    "        for token in tokens:\n",
    "            if token not in doc_freq:\n",
    "                doc_freq[token] = {i}\n",
    "          \n",
    "            else:\n",
    "                doc_freq[token].add(i)\n",
    "\n",
    "    for token in doc_freq:\n",
    "        doc_freq[token] = len(doc_freq[token])\n",
    "\n",
    "    return doc_freq\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WY4YzoZNNyZ2"
   },
   "outputs": [],
   "source": [
    "#### Compute df based on the two configs\n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1])\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2])\n",
    "\n",
    "def get_df(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: df_1,\n",
    "        2: df_2\n",
    "    }[index_set]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Tjt36QvlNyZ3"
   },
   "outputs": [],
   "source": [
    "def tfidf_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using tf-idf. \n",
    "        Note #1: You have to use the `get_index` (and the `get_df`) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set)\n",
    "    df = get_df(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    n_doc = len(doc_repr_1) if index_set == 1 else len(doc_repr_2)\n",
    "\n",
    "    tfidf_dict = {}\n",
    "\n",
    "    for q in processed_query:\n",
    "\n",
    "        if q not in index:\n",
    "            continue \n",
    "\n",
    "        for doc_id, tf in index[q]:        \n",
    "            if doc_id not in tfidf_dict:\n",
    "                    tfidf_dict[doc_id] = 0\n",
    "            \n",
    "            tfidf_dict[doc_id] += tf*np.log(n_doc/df[q])\n",
    "\n",
    "    sorted_result = sorted(tfidf_dict.items(), key=lambda tup: tup[1], reverse = True)\n",
    "\n",
    "    return sorted_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "roRYISlkNyZ3"
   },
   "outputs": [],
   "source": [
    "#### Document length for normalization\n",
    "\n",
    "def doc_lengths(documents):\n",
    "    doc_lengths = {doc_id:len(doc) for (doc_id, doc) in documents}\n",
    "    return doc_lengths\n",
    "\n",
    "doc_lengths_1 = doc_lengths(doc_repr_1)\n",
    "doc_lengths_2 = doc_lengths(doc_repr_2)\n",
    "\n",
    "def get_doc_lengths(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_lengths_1,\n",
    "        2: doc_lengths_2\n",
    "    }[index_set]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KaNDnMLPNyZ4"
   },
   "outputs": [],
   "source": [
    "def naive_ql_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a naive QL model. \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    unigram_probs = {}\n",
    "\n",
    "\n",
    "    for i, q in enumerate(processed_query):\n",
    "      if q not in index:\n",
    "        continue\n",
    "      \n",
    "      if i > 0:\n",
    "        tf_dicts = dict(index[q])\n",
    "        for doc_id in unigram_probs:\n",
    "          if doc_id in tf_dicts:\n",
    "            unigram_probs[doc_id] *= 1.0 * tf_dicts[doc_id] / doc_lengths[doc_id]  \n",
    "        \n",
    "          else:\n",
    "            unigram_probs[doc_id] = 0\n",
    "\n",
    "      else:    \n",
    "        for doc_id, tf in index[processed_query[0]]:\n",
    "          unigram_probs[doc_id] = tf / doc_lengths[doc_id] \n",
    "\n",
    "    sorted_rank = sorted(unigram_probs.items(), key = lambda d: d[1], reverse = True)\n",
    "    \n",
    "    return sorted_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "I34arRELNyZ4"
   },
   "outputs": [],
   "source": [
    "def get_doc_ids(query, index_set):\n",
    "  \"return doc_id list of documents that contain the query terms\"\n",
    "  index = get_index(index_set)\n",
    "  doc_ids = []\n",
    "  for q in query:\n",
    "    if q not in index:\n",
    "      continue\n",
    "      \n",
    "    for doc_id, _ in index[q]:\n",
    "      if doc_id not in doc_ids:\n",
    "        doc_ids.append(doc_id)\n",
    "    \n",
    "  return doc_ids\n",
    "\n",
    "\n",
    "def ql_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using a QL model \n",
    "        with Jelinek-Mercer Smoothing (set smoothing=0.1). \n",
    "        \n",
    "        \n",
    "        Note #1: You have to use the `get_index` (and get_doc_lengths) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        \n",
    "        \n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    index = get_index(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    doc_ids = get_doc_ids(processed_query, index_set)\n",
    "    cl = sum(doc_lengths.values())\n",
    "    lamb = 0.1\n",
    "    unigram_probs = dict(zip(doc_ids, np.zeros(len(doc_ids))))\n",
    "    \n",
    "    for i, q in enumerate(processed_query):\n",
    "      if q not in index:\n",
    "        continue\n",
    "      \n",
    "      tf_dict = dict(index[q])\n",
    "      cf = sum(tf_dict.values())\n",
    "\n",
    "      for doc_id in doc_ids:\n",
    "        tf = tf_dict[doc_id] if doc_id in tf_dict else 0\n",
    "        unigram_probs[doc_id] += np.log((1 - lamb) * tf / doc_lengths[doc_id] + lamb * cf / cl)\n",
    "          \n",
    "    sorted_rank = sorted(unigram_probs.items(), key = lambda d: d[1], reverse = True)\n",
    "    return sorted_rank\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "u0j_qGA1NyZ5"
   },
   "outputs": [],
   "source": [
    "def bm25_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. Use k_1 = 1.5 and b = 0.75\n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        \n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    index = get_index(index_set)\n",
    "    df = get_df(index_set)\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    k_1, b = 1.5, 0.75\n",
    "    bm25_dict = {}\n",
    "    dl_avg = 1.0 * sum(doc_lengths.values()) / len(doc_lengths)\n",
    " \n",
    "    for q in processed_query:\n",
    "      if q not in index:\n",
    "        continue\n",
    "      \n",
    "      for doc_id, tf in index[q]:\n",
    "        if doc_id not in bm25_dict:\n",
    "          bm25_dict[doc_id] = 0\n",
    "        \n",
    "        idf = np.log(len(doc_lengths)/df[q])\n",
    "        bm25_dict[doc_id] += idf * (k_1 + 1) * tf / (k_1 * (1-b + b * doc_lengths[doc_id]/ dl_avg) + tf)\n",
    "    \n",
    "    sorted_rank = sorted(bm25_dict.items(), key = lambda d: d[1], reverse = True)\n",
    "    return sorted_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yPeZ8e4yNyZ6"
   },
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50,
     "referenced_widgets": [
      "01458fb095424240baee28c6b347555c",
      "ac7ec29c04f64e2f962476650c3c4e68",
      "33dc764cdf144f79bd8d3ef202ff79bd"
     ]
    },
    "id": "SN9U7xe8NyZ6",
    "outputId": "2eebffe8-2945-4adc-d4d4-7b2193207977"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a0558c0d084beab97339b7a9630f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_fn = bm25_search\n",
    "index_set = 1\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"Searching for: '{sender.value}'\")\n",
    "    \n",
    "    results = make_results(sender.value, search_fn, index_set)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Dbo0p7VINyZ7"
   },
   "outputs": [],
   "source": [
    "def read_qrels(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads the qrels.text file. \n",
    "        Output: A dictionary: query_id -> [list of relevant documents]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_f = open(os.path.join(root_folder, \"qrels.text\"), 'r')\n",
    "    query_dic = {}\n",
    "\n",
    "    for line in query_f:\n",
    "      q_id, doc_id, _, _ = line.split()\n",
    "      \n",
    "      # make queries and qrels consistent in query_id ('01'->'1')\n",
    "      q_id = str(int(q_id))\n",
    "      \n",
    "      if q_id not in query_dic:\n",
    "        query_dic[q_id] = []\n",
    "      query_dic[q_id].append(doc_id)\n",
    "    \n",
    "    return query_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "nnlKssKdNyZ7"
   },
   "outputs": [],
   "source": [
    "def precision_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Precision@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), \n",
    "                    with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Precision@K\n",
    "    \"\"\"\n",
    "    relevant_cnt = 0\n",
    "\n",
    "    for i, (doc_id, _) in enumerate(results):\n",
    "      if doc_id in relevant_docs:\n",
    "        relevant_cnt += 1\n",
    "      if i == k - 1:\n",
    "        break\n",
    "\n",
    "    return relevant_cnt / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "OTbYzC-WNyZ8"
   },
   "outputs": [],
   "source": [
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Recall@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Recall@K\n",
    "    \"\"\"\n",
    "    relevant_cnt = 0\n",
    "\n",
    "    for i, (doc_id, _) in enumerate(results):\n",
    "      if doc_id in relevant_docs:\n",
    "        relevant_cnt += 1\n",
    "      if i == k - 1:\n",
    "        break\n",
    "\n",
    "    return relevant_cnt / len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "wXBvfQ0QNyZ8"
   },
   "outputs": [],
   "source": [
    "def average_precision(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute Average Precision (for a single query - the results are \n",
    "        averaged across queries to get MAP in the next few cells)\n",
    "        Hint: You can use the recall_k and precision_k functions here!\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: Average Precision\n",
    "    \"\"\"\n",
    "    relevant_cnt = 0\n",
    "    sum = 0\n",
    "    search_cnt = 0\n",
    "\n",
    "    while relevant_cnt < len(relevant_docs) and search_cnt < len(results):\n",
    "      doc_id, _ = results[search_cnt]\n",
    "      search_cnt += 1\n",
    "      if doc_id in relevant_docs:\n",
    "        relevant_cnt += 1\n",
    "        sum += relevant_cnt / search_cnt\n",
    "\n",
    "    return sum / len(relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "TfcXuKA3NyZ8"
   },
   "outputs": [],
   "source": [
    "def err(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute the expected reciprocal rank.\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: ERR\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    err_score = 0\n",
    "    r_prod = 1\n",
    "\n",
    "    for i, (doc_id, _) in enumerate(results):\n",
    "      if doc_id in relevant_docs:\n",
    "        err_score += 1/(i+1) * r_prod * 0.5\n",
    "        r_prod *= 1 - 0.5\n",
    "    \n",
    "    return err_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "6OP_M-QhNyZ9"
   },
   "outputs": [],
   "source": [
    "#### metrics@k functions\n",
    "\n",
    "recall_at_1 = partial(recall_k, k=1)\n",
    "recall_at_5 = partial(recall_k, k=5)\n",
    "recall_at_10 = partial(recall_k, k=10)\n",
    "precision_at_1 = partial(precision_k, k=1)\n",
    "precision_at_5 = partial(precision_k, k=5)\n",
    "precision_at_10 = partial(precision_k, k=10)\n",
    "\n",
    "\n",
    "list_of_metrics = [\n",
    "    (\"ERR\", err),\n",
    "    (\"MAP\", average_precision),\n",
    "    (\"Recall@1\",recall_at_1),\n",
    "    (\"Recall@5\", recall_at_5),\n",
    "    (\"Recall@10\", recall_at_10),\n",
    "    (\"Precision@1\", precision_at_1),\n",
    "    (\"Precision@5\", precision_at_5),\n",
    "    (\"Precision@10\", precision_at_10)]\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "7hL0ypKWNyZ9"
   },
   "outputs": [],
   "source": [
    "#### Evaluate a search function\n",
    "\n",
    "list_of_search_fns = [\n",
    "    (\"BOW\", bow_search),\n",
    "    (\"TF-IDF\", tfidf_search),\n",
    "    (\"NaiveQL\", naive_ql_search),\n",
    "    (\"QL\", ql_search),\n",
    "    (\"BM25\", bm25_search)\n",
    "]\n",
    "\n",
    "def evaluate_search_fn(search_fn, metric_fns, index_set=None):\n",
    "    # build a dict query_id -> query \n",
    "    queries_by_id = dict((q[0], q[1]) for q in queries)\n",
    "    \n",
    "    metrics = {}\n",
    "    for metric, metric_fn in metric_fns:\n",
    "        metrics[metric] = np.zeros(len(qrels), dtype=np.float32)\n",
    "    \n",
    "    for i, (query_id, relevant_docs) in enumerate(qrels.items()):\n",
    "        query = queries_by_id[query_id]\n",
    "        if index_set:\n",
    "            results = search_fn(query, index_set)\n",
    "        else:\n",
    "            results = search_fn(query)\n",
    "        \n",
    "        for metric, metric_fn in metric_fns:\n",
    "            metrics[metric][i] = metric_fn(results, relevant_docs)\n",
    "\n",
    "    \n",
    "    \n",
    "    final_dict = {}\n",
    "    for metric, metric_vals in metrics.items():\n",
    "        final_dict[metric] = metric_vals.mean()\n",
    "    \n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4XQhhFFZNyZ-"
   },
   "outputs": [],
   "source": [
    "def dot(vec_1,vec_2): \n",
    "    \"\"\"\n",
    "        vec_1 and vec_2 are of the form: [(int, float), (int, float), ...]\n",
    "        Return the dot product of two such vectors, computed only on the floats\n",
    "        You can assume that the lengths of the vectors are the same, and the dimensions are aligned \n",
    "            i.e you won't get: vec_1 = [(1, 0.2)] ; vec_2 = [(2, 0.3)] \n",
    "                                (dimensions are unaligned and lengths are different)\n",
    "    \"\"\"\n",
    "    return sum( [vec_1[i][1]*vec_2[i][1] for i in range(len(vec_1))] )\n",
    "\n",
    "\n",
    "def cosine_sim(vec_1, vec_2):\n",
    "    return dot(vec_1, vec_2)/((np.sqrt(dot(vec_1, vec_1)) * np.sqrt(dot(vec_2, vec_2)))+1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "NlHxaCkCNyZ-"
   },
   "outputs": [],
   "source": [
    "class VectorSpaceRetrievalModel:\n",
    "    \"\"\"\n",
    "        Parent class for Dense Vector Retrieval models\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_repr):\n",
    "        \"\"\"\n",
    "            document_collection: \n",
    "                [\n",
    "                    (doc_id_1, [token 1, token 2, ...]), \n",
    "                    (doc_id_2, [token 1, token 2, ....]) \n",
    "                    ...\n",
    "                ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.doc_repr = doc_repr\n",
    "        self.documents = [_[1] for _ in self.doc_repr]\n",
    "        \n",
    "        # construct a dictionary\n",
    "        self.dictionary = Dictionary(self.documents)\n",
    "        # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "        self.dictionary.filter_extremes(no_below=10)\n",
    "        self.corpus = [self.dictionary.doc2bow(doc) for doc in self.documents]\n",
    "    \n",
    "        # Make a index to word dictionary.\n",
    "        temp = self.dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "        self.id2word = self.dictionary.id2token\n",
    "        \n",
    "        # this is set by the train_model function\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        vectors = {}\n",
    "        for (doc_id, _), cc in zip(self.doc_repr, self.corpus):\n",
    "            vectors[doc_id] = self.model[cc]\n",
    "        return vectors\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        # Note the use of config_2 here!\n",
    "        query = process_text(query, **config_2)\n",
    "        query_vector = self.dictionary.doc2bow(query)\n",
    "        return self.model[query_vector]\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "            Trains a model and sets the 'self.model' variable. \n",
    "            Make sure to use the variables created in the __init__ method.\n",
    "            e.g the variables which may be useful: {corpus, dictionary, id2word}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "sv5wcwXQNyZ_"
   },
   "outputs": [],
   "source": [
    "class LsiRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        self.num_topics = 100\n",
    "        self.chunksize = 2000\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.model = LsiModel(self.corpus, num_topics=self.num_topics, id2word=self.id2word, chunksize=self.chunksize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "t29jqKUYNyZ_"
   },
   "outputs": [],
   "source": [
    "class DenseRetrievalRanker:\n",
    "    def __init__(self, vsrm, similarity_fn):\n",
    "        \"\"\"\n",
    "            vsrm: instance of `VectorSpaceRetrievalModel`\n",
    "            similarity_fn: function instance that takes in two vectors \n",
    "                            and returns a similarity score e.g cosine_sim defined earlier\n",
    "        \"\"\"\n",
    "        self.vsrm = vsrm \n",
    "        self.vectorized_documents = self.vsrm.vectorize_documents()\n",
    "        self.similarity_fn = similarity_fn\n",
    "    \n",
    "    def _compute_sim(self, query_vector):\n",
    "        \"\"\"\n",
    "            Compute the similarity of `query_vector` to documents in \n",
    "            `self.vectorized_documents` using `self.similarity_fn`\n",
    "            Returns a list of (doc_id, score) tuples\n",
    "        \"\"\"\n",
    "        empty_list = []\n",
    "\n",
    "        for key in self.vectorized_documents:\n",
    "          doc_id = key\n",
    "          document_single = self.vectorized_documents.get(key)\n",
    "\n",
    "          if document_single != [] and query_vector != []:\n",
    "            score = self.similarity_fn(document_single,query_vector) # compute similary between the query vector to each vectorized document\n",
    "            empty_list.append(tuple((doc_id, score)))\n",
    "          else:\n",
    "            score = 0\n",
    "            empty_list.append(tuple((doc_id, score)))\n",
    "\n",
    "\n",
    "        return empty_list\n",
    "    \n",
    "    def search(self, query):\n",
    "        scores = self._compute_sim(self.vsrm.vectorize_query(query))\n",
    "        scores.sort(key=lambda _:-_[1])\n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "56084253838740e2b8c958a439e997c6",
      "5a95c4a6dbe24f91b2c5b7a65376b590",
      "4ec6170bbcae48dc8c41adba6f85f5da"
     ]
    },
    "id": "TsZNCpFVNyaA",
    "outputId": "8b9ecd1d-0cc3-4554-c38f-18bf672f7e81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bc48667ffa403488c4a17a7148a391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test your LSI model\n",
    "search_fn = drm_lsi.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def make_results_2(query, search_fn):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "\n",
    "def handle_submit_2(sender):\n",
    "    print(f\"Searching for: '{sender.value}' (SEARCH FN: {search_fn})\")\n",
    "    \n",
    "    results = make_results_2(sender.value, search_fn)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "b3z0cgEPNyaA"
   },
   "outputs": [],
   "source": [
    "def jenson_shannon_divergence(vec_1, vec_2, assert_prob=False):\n",
    "    \"\"\"\n",
    "        Computes the Jensen-Shannon divergence between two probability distributions. \n",
    "        NOTE: DO NOT RETURN 1 - JSD here, that is handled by the next function which is already implemented! \n",
    "        The inputs are *gensim* vectors - same as the vectors for the cosine_sim function\n",
    "        assert_prob is a flag that checks if the inputs are proper probability distributions \n",
    "            i.e they sum to 1 and are positive - use this to check your inputs if needed. \n",
    "                (This is optional to implement, but recommended - \n",
    "                you can the default to False to save a few ms off the runtime)\n",
    "    \"\"\"\n",
    "    _vec_1 = np.asarray(vec_1) / sum(n for _, n in vec_1)\n",
    "    _vec_2 = np.asarray(vec_2) / sum(n for _, n in vec_2)\n",
    "    _avg = 0.5 * (_vec_1 + _vec_2)\n",
    "    \n",
    "    def KL(a,b):\n",
    "      a = np.asarray([x[1] for x in a], dtype=np.float)\n",
    "      b = np.asarray([x[1] for x in b], dtype=np.float)\n",
    "\n",
    "      return np.sum(np.where(a != 0, a * np.log2(a / b), 0))\n",
    "\n",
    "    return 0.5 * (KL(_vec_1, _avg) + KL(_vec_2, _avg))\n",
    "\n",
    "def jenson_shannon_sim(vec_1, vec_2, assert_prob=False):\n",
    "    return 1 - jenson_shannon_divergence(vec_1, vec_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "fTh0-seuNyaA"
   },
   "outputs": [],
   "source": [
    "class LdaRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        # use these parameters in the train_model method\n",
    "        self.num_topics = 100\n",
    "        self.chunksize = 2000\n",
    "        self.passes = 20\n",
    "        self.iterations = 400\n",
    "        self.eval_every = 10\n",
    "        \n",
    "        # this is need to get full vectors\n",
    "        self.minimum_probability=0.0\n",
    "        self.alpha='auto'\n",
    "        self.eta='auto'\n",
    "    \n",
    "    \n",
    "    def train_model(self):\n",
    "        self.model = LdaModel(self.corpus, self.num_topics, id2word=self.id2word, chunksize=self.chunksize,\n",
    "                              passes=self.passes, iterations=self.iterations, eval_every=self.eval_every, \n",
    "                              minimum_probability=self.minimum_probability, alpha=self.alpha, eta=self.eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5ba4d8180cf1497d8978117ac7322416",
      "e9537ba0a1184f7882569dd56d62558a",
      "73825775b25443b0bcab415fd3fcf5aa"
     ]
    },
    "id": "_Mnqktc8NyaB",
    "outputId": "f346c60a-a928-4092-ab21-e3273ffa38cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ea83dc30f6498fb8242d1226c705a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drm_lda = DenseRetrievalRanker(lda, jenson_shannon_sim)\n",
    "\n",
    "search_fn = drm_lda.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hbf-bjj_NyaB",
    "outputId": "3679d73c-894a-4d10-8cc4-05be517011a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:52:44,336 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-19 22:52:44,438 : INFO : built Dictionary(5937 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...) from 3204 documents (total 115969 corpus positions)\n",
      "2021-02-19 22:52:44,443 : INFO : discarding 4740 tokens: [('repeat', 8), ('glossari', 7), ('inspect', 8), ('uncol', 2), ('rung', 9), ('secant', 2), ('.', 1603), ('acceler', 6), ('diverg', 3), ('induc', 9)]...\n",
      "2021-02-19 22:52:44,444 : INFO : keeping 1197 tokens which were in no less than 10 and no more than 1602 (=50.0%) documents\n",
      "2021-02-19 22:52:44,446 : INFO : resulting dictionary: Dictionary(1197 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...)\n",
      "2021-02-19 22:52:44,608 : INFO : collecting all words and their counts\n",
      "2021-02-19 22:52:44,608 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-19 22:52:44,624 : INFO : collected 5937 word types from a corpus of 115969 raw words and 3204 sentences\n",
      "2021-02-19 22:52:44,624 : INFO : Loading a fresh vocabulary\n",
      "2021-02-19 22:52:44,633 : INFO : effective_min_count=1 retains 5937 unique words (100% of original 5937, drops 0)\n",
      "2021-02-19 22:52:44,633 : INFO : effective_min_count=1 leaves 115969 word corpus (100% of original 115969, drops 0)\n",
      "2021-02-19 22:52:44,648 : INFO : deleting the raw counts dictionary of 5937 items\n",
      "2021-02-19 22:52:44,649 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2021-02-19 22:52:44,649 : INFO : downsampling leaves estimated 92346 word corpus (79.6% of prior 115969)\n",
      "2021-02-19 22:52:44,658 : INFO : estimated required memory for 5937 words and 100 dimensions: 7718100 bytes\n",
      "2021-02-19 22:52:44,658 : INFO : resetting layer weights\n",
      "2021-02-19 22:52:45,633 : INFO : training model with 3 workers on 5937 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-19 22:52:45,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:52:45,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:52:45,692 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:52:45,692 : INFO : EPOCH - 1 : training on 115969 raw words (92259 effective words) took 0.1s, 1651471 effective words/s\n",
      "2021-02-19 22:52:45,748 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:52:45,753 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:52:45,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:52:45,755 : INFO : EPOCH - 2 : training on 115969 raw words (92243 effective words) took 0.1s, 1633104 effective words/s\n",
      "2021-02-19 22:52:45,811 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:52:45,815 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:52:45,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:52:45,816 : INFO : EPOCH - 3 : training on 115969 raw words (92346 effective words) took 0.1s, 1652945 effective words/s\n",
      "2021-02-19 22:52:45,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:52:45,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:52:45,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:52:45,876 : INFO : EPOCH - 4 : training on 115969 raw words (92349 effective words) took 0.1s, 1669036 effective words/s\n",
      "2021-02-19 22:52:45,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:52:45,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:52:45,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:52:45,935 : INFO : EPOCH - 5 : training on 115969 raw words (92313 effective words) took 0.1s, 1713705 effective words/s\n",
      "2021-02-19 22:52:45,935 : INFO : training on a 579845 raw words (461510 effective words) took 0.3s, 1529180 effective words/s\n",
      "2021-02-19 22:52:45,936 : INFO : saving Word2Vec object under word2vec-google-news-300.model, separately None\n",
      "2021-02-19 22:52:45,936 : INFO : not storing attribute vectors_norm\n",
      "2021-02-19 22:52:45,937 : INFO : not storing attribute cum_table\n",
      "2021-02-19 22:52:45,978 : INFO : saved word2vec-google-news-300.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.10895367),\n",
       " (1, 0.12217637),\n",
       " (2, -0.07480104),\n",
       " (3, 0.19750515),\n",
       " (4, -0.013826887),\n",
       " (5, 0.069984645),\n",
       " (6, 0.014467405),\n",
       " (7, 0.059876934),\n",
       " (8, 0.11729815),\n",
       " (9, 0.16905047),\n",
       " (10, 0.10974939),\n",
       " (11, -0.026893523),\n",
       " (12, 0.03642345),\n",
       " (13, 0.067993626),\n",
       " (14, -0.024139423),\n",
       " (15, 0.06633035),\n",
       " (16, 0.1302095),\n",
       " (17, -0.063747145),\n",
       " (18, 0.085926525),\n",
       " (19, -0.031735405),\n",
       " (20, 0.08511715),\n",
       " (21, -0.075270005),\n",
       " (22, -0.0049473816),\n",
       " (23, -0.1396215),\n",
       " (24, 0.12844622),\n",
       " (25, -0.0039034665),\n",
       " (26, -0.13227499),\n",
       " (27, 0.11614388),\n",
       " (28, 0.26218578),\n",
       " (29, -0.04970734),\n",
       " (30, -0.033200137),\n",
       " (31, 0.12476612),\n",
       " (32, -0.14957774),\n",
       " (33, 0.16482496),\n",
       " (34, 0.08254256),\n",
       " (35, -0.14865686),\n",
       " (36, 0.038189203),\n",
       " (37, 0.016183285),\n",
       " (38, -0.06224189),\n",
       " (39, -0.10534956),\n",
       " (40, 0.085742265),\n",
       " (41, -0.01841159),\n",
       " (42, 0.116705224),\n",
       " (43, 0.035525147),\n",
       " (44, -0.017305037),\n",
       " (45, 0.12868994),\n",
       " (46, 0.0064053936),\n",
       " (47, -0.030771768),\n",
       " (48, -0.07658876),\n",
       " (49, 0.087660454),\n",
       " (50, 0.03327767),\n",
       " (51, -0.15947162),\n",
       " (52, -0.01246604),\n",
       " (53, -0.0879805),\n",
       " (54, 0.026656978),\n",
       " (55, -0.103733644),\n",
       " (56, -0.20465983),\n",
       " (57, -0.06037432),\n",
       " (58, 0.02414716),\n",
       " (59, 0.10907201),\n",
       " (60, -0.22310889),\n",
       " (61, 0.04525013),\n",
       " (62, 0.06497768),\n",
       " (63, -0.16150714),\n",
       " (64, 0.041646574),\n",
       " (65, -0.031070583),\n",
       " (66, -0.11565049),\n",
       " (67, 0.17899224),\n",
       " (68, -0.10456918),\n",
       " (69, 0.020228583),\n",
       " (70, -0.011115231),\n",
       " (71, -0.06466617),\n",
       " (72, 0.006222906),\n",
       " (73, 0.17054154),\n",
       " (74, 0.08914998),\n",
       " (75, -0.028424831),\n",
       " (76, 0.02643886),\n",
       " (77, 0.0052676797),\n",
       " (78, 0.07625557),\n",
       " (79, 0.079055324),\n",
       " (80, 0.10724217),\n",
       " (81, 0.005724199),\n",
       " (82, 0.010440751),\n",
       " (83, -0.03852699),\n",
       " (84, 0.06753365),\n",
       " (85, 0.017771222),\n",
       " (86, -0.14591461),\n",
       " (87, 0.089967504),\n",
       " (88, -0.16306537),\n",
       " (89, 0.08617493),\n",
       " (90, -0.2104472),\n",
       " (91, -0.0012645418),\n",
       " (92, -0.22114657),\n",
       " (93, 0.113700196),\n",
       " (94, -0.009764409),\n",
       " (95, 0.026633639),\n",
       " (96, -0.10216944),\n",
       " (97, -0.039209332),\n",
       " (98, -0.023183368),\n",
       " (99, 0.07755286)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class W2VRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        self.size = 100 \n",
    "        self.min_count = 1\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the W2V model\n",
    "        \"\"\"\n",
    "        self.model = Word2Vec(self.documents, size=self.size, min_count = self.min_count)\n",
    "        self.model.save(\"word2vec-google-news-300.model\")\n",
    "        \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        vectors = {}\n",
    "        for (doc_id, _), cc in zip(self.doc_repr, self.documents):\n",
    "          vector_dim = self.model.vector_size\n",
    "          arr = np.empty((0,vector_dim), dtype='f')\n",
    "\n",
    "          for wrd in cc:\n",
    "            if wrd in self.model.wv.vocab:\n",
    "              word_array = self.model.wv[wrd]\n",
    "              norm = np.linalg.norm(word_array)\n",
    "              word_array = (word_array/norm).reshape(1, -1)\n",
    "              arr = np.append(arr,np.array(word_array), axis=0)\n",
    "            else:\n",
    "              word_array = np.zeros(self.size).reshape(1, -1)\n",
    "              arr = np.append(arr,np.array(word_array), axis=0)\n",
    "\n",
    "          list1 = np.mean(arr, axis=0)\n",
    "          list2 = list(range(self.size))\n",
    "          vectors[doc_id] = list(zip(list2, list1)) # save vectorized query for each doc\n",
    "        return vectors\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        \"\"\"\n",
    "        Vectorizes the query using the W2V model\n",
    "        \"\"\"\n",
    "        query = process_text(query, **config_2)\n",
    "        vector_dim = self.model.vector_size\n",
    "        arr = np.empty((0,vector_dim), dtype='f')\n",
    "        \n",
    "        for wrd in query:\n",
    "          if wrd in self.model.wv.vocab:\n",
    "            word_array = self.model.wv[wrd] # infer vector for each word\n",
    "\n",
    "            norm = np.linalg.norm(word_array)\n",
    "            word_array = (word_array/norm).reshape(1, -1) # normalize the inferred vector\n",
    "\n",
    "            arr = np.append(arr,np.array(word_array), axis=0)\n",
    "          else:\n",
    "            word_array = np.zeros(self.size).reshape(1, -1) # if the word is not present, return 0 for all dimension\n",
    "\n",
    "            arr = np.append(arr,np.array(word_array), axis=0)\n",
    "\n",
    "        list1 = np.mean(arr, axis=0) # average over each dimension\n",
    "        list2 = list(range(self.size))\n",
    "\n",
    "        return list(zip(list2, list1))\n",
    "      \n",
    "    \n",
    "class W2VPretrainedRetrievalModel(W2VRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        self.model_name = \"word2vec-google-news-300\"\n",
    "        self.size = 300\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Loads the pretrained model\n",
    "        \"\"\"\n",
    "        self.model = g_downloader.load(self.model_name)\n",
    "\n",
    "w2v = W2VRetrievalModel(doc_repr_2)\n",
    "w2v.train_model()\n",
    "\n",
    "w2v.vectorize_query(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frVpdla6NyaB",
    "outputId": "546495d6-e8f4-4373-bd02-48a9d0a1bbfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:52:45,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-19 22:52:46,092 : INFO : built Dictionary(5937 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...) from 3204 documents (total 115969 corpus positions)\n",
      "2021-02-19 22:52:46,099 : INFO : discarding 4740 tokens: [('repeat', 8), ('glossari', 7), ('inspect', 8), ('uncol', 2), ('rung', 9), ('secant', 2), ('.', 1603), ('acceler', 6), ('diverg', 3), ('induc', 9)]...\n",
      "2021-02-19 22:52:46,099 : INFO : keeping 1197 tokens which were in no less than 10 and no more than 1602 (=50.0%) documents\n",
      "2021-02-19 22:52:46,102 : INFO : resulting dictionary: Dictionary(1197 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:56:01,189 : INFO : word2vec-google-news-300 downloaded\n",
      "2021-02-19 22:56:01,199 : INFO : loading projection weights from /Users/xinyichen/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2021-02-19 22:56:42,420 : INFO : loaded (3000000, 300) matrix from /Users/xinyichen/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "<ipython-input-88-07a28d4b976b>:56: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if wrd in self.model.wv.vocab:\n",
      "<ipython-input-88-07a28d4b976b>:57: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word_array = self.model.wv[wrd] # infer vector for each word\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, -0.05876739),\n",
       " (1, -0.06762275),\n",
       " (2, -0.037232764),\n",
       " (3, -0.04628938),\n",
       " (4, 0.041257925),\n",
       " (5, -0.017006315),\n",
       " (6, 0.020125818),\n",
       " (7, -0.05635229),\n",
       " (8, 0.08090579),\n",
       " (9, -0.055547256),\n",
       " (10, -0.0072452943),\n",
       " (11, 0.013283039),\n",
       " (12, 0.039245345),\n",
       " (13, -0.043673024),\n",
       " (14, -0.07003785),\n",
       " (15, 0.016905688),\n",
       " (16, -0.108679414),\n",
       " (17, -0.0026037777),\n",
       " (18, -0.073257975),\n",
       " (19, -0.09901902),\n",
       " (20, 0.1449059),\n",
       " (21, -0.0050314544),\n",
       " (22, -0.066817716),\n",
       " (23, -0.04971077),\n",
       " (24, 0.017811349),\n",
       " (25, 0.04528309),\n",
       " (26, 0.021635255),\n",
       " (27, 0.07366049),\n",
       " (28, -0.05997494),\n",
       " (29, 0.056754805),\n",
       " (30, -0.033811375),\n",
       " (31, -0.116729744),\n",
       " (32, -0.044880573),\n",
       " (33, -0.11914484),\n",
       " (34, 0.02978621),\n",
       " (35, -0.019522043),\n",
       " (36, 0.0166038),\n",
       " (37, 0.027974887),\n",
       " (38, 0.048100706),\n",
       " (39, 0.00034276783),\n",
       " (40, 0.028176146),\n",
       " (41, 0.04950951),\n",
       " (42, -0.03642773),\n",
       " (43, 0.13927066),\n",
       " (44, -0.018415123),\n",
       " (45, -0.012427692),\n",
       " (46, 0.0031698162),\n",
       " (47, -0.008704416),\n",
       " (48, -0.10465425),\n",
       " (49, 0.061585),\n",
       " (50, 0.16422667),\n",
       " (51, 0.008905674),\n",
       " (52, -0.009559764),\n",
       " (53, 0.0026163564),\n",
       " (54, 0.0033962317),\n",
       " (55, -0.06963533),\n",
       " (56, 0.01539625),\n",
       " (57, -0.026163563),\n",
       " (58, 0.04830196),\n",
       " (59, -0.061182488),\n",
       " (60, -0.032402568),\n",
       " (61, 0.15215118),\n",
       " (62, -0.09257876),\n",
       " (63, 0.07164791),\n",
       " (64, 0.005710701),\n",
       " (65, 0.077283144),\n",
       " (66, -0.041660443),\n",
       " (67, 0.015597509),\n",
       " (68, 0.0008742152),\n",
       " (69, -0.021132108),\n",
       " (70, 0.039647862),\n",
       " (71, -0.023547206),\n",
       " (72, 0.043673024),\n",
       " (73, -0.008654102),\n",
       " (74, 0.0664152),\n",
       " (75, 0.040251635),\n",
       " (76, 0.015798766),\n",
       " (77, 0.09056618),\n",
       " (78, 0.043874282),\n",
       " (79, 0.03542144),\n",
       " (80, 0.07406301),\n",
       " (81, -0.03421389),\n",
       " (82, -0.013685556),\n",
       " (83, 0.016905688),\n",
       " (84, -0.026767338),\n",
       " (85, 0.00820127),\n",
       " (86, 0.055144742),\n",
       " (87, -0.040251635),\n",
       " (88, -0.079698235),\n",
       " (89, -0.09700644),\n",
       " (90, 0.07849069),\n",
       " (91, 0.0010251588),\n",
       " (92, -0.006490576),\n",
       " (93, 0.013283039),\n",
       " (94, -0.09056618),\n",
       " (95, -0.037635278),\n",
       " (96, 0.008754731),\n",
       " (97, 0.06520765),\n",
       " (98, 0.12317),\n",
       " (99, -0.1352455),\n",
       " (100, 0.023647835),\n",
       " (101, -0.019119527),\n",
       " (102, 0.0466919),\n",
       " (103, -0.0418617),\n",
       " (104, 0.07446553),\n",
       " (105, -0.010364796),\n",
       " (106, -0.06842778),\n",
       " (107, -0.039647862),\n",
       " (108, 0.00774844),\n",
       " (109, -0.08010075),\n",
       " (110, 0.061585),\n",
       " (111, -0.02334595),\n",
       " (112, -0.04407554),\n",
       " (113, -0.06198752),\n",
       " (114, -0.07245295),\n",
       " (115, 0.04649064),\n",
       " (116, 0.020327076),\n",
       " (117, 0.09056618),\n",
       " (118, 0.13122033),\n",
       " (119, 0.064805135),\n",
       " (120, -0.120754905),\n",
       " (121, 0.04327051),\n",
       " (122, -0.122364976),\n",
       " (123, -0.0055346),\n",
       " (124, 0.037434023),\n",
       " (125, 0.005182398),\n",
       " (126, -0.021232737),\n",
       " (127, -0.053132158),\n",
       " (128, 0.0059622736),\n",
       " (129, 0.04971077),\n",
       " (130, -0.041660443),\n",
       " (131, -0.045886863),\n",
       " (132, 0.026364822),\n",
       " (133, -0.021836512),\n",
       " (134, -0.017811349),\n",
       " (135, -0.0053333417),\n",
       " (136, -0.011371087),\n",
       " (137, -0.040050376),\n",
       " (138, -0.083320886),\n",
       " (139, 0.05071706),\n",
       " (140, -0.017509462),\n",
       " (141, 0.07164791),\n",
       " (142, -0.005635229),\n",
       " (143, 0.0418617),\n",
       " (144, -0.00038679305),\n",
       " (145, -0.014691847),\n",
       " (146, 0.12397504),\n",
       " (147, -0.028176146),\n",
       " (148, -0.02314469),\n",
       " (149, 0.01549688),\n",
       " (150, 0.027169853),\n",
       " (151, 0.0166038),\n",
       " (152, 0.004905668),\n",
       " (153, 0.039647862),\n",
       " (154, 0.15134615),\n",
       " (155, -0.11511968),\n",
       " (156, -0.021433996),\n",
       " (157, -0.04407554),\n",
       " (158, -0.055547256),\n",
       " (159, -0.019018898),\n",
       " (160, 0.10304419),\n",
       " (161, 0.0023396264),\n",
       " (162, -0.005937116),\n",
       " (163, -0.012779894),\n",
       " (164, -0.024855385),\n",
       " (165, 0.043673024),\n",
       " (166, 0.15617634),\n",
       " (167, 0.02425161),\n",
       " (168, 0.05071706),\n",
       " (169, -0.04749693),\n",
       " (170, 0.061585),\n",
       " (171, -0.06762275),\n",
       " (172, -0.09620141),\n",
       " (173, -0.03200005),\n",
       " (174, 0.011018885),\n",
       " (175, -0.008704416),\n",
       " (176, 0.08010075),\n",
       " (177, -0.02314469),\n",
       " (178, -0.0026540922),\n",
       " (179, -0.07003785),\n",
       " (180, -0.048905738),\n",
       " (181, -0.027773628),\n",
       " (182, -0.028377403),\n",
       " (183, -0.035823956),\n",
       " (184, -0.0418617),\n",
       " (185, 0.0058364873),\n",
       " (186, -0.08291837),\n",
       " (187, 0.040452894),\n",
       " (188, -0.004226422),\n",
       " (189, 0.077283144),\n",
       " (190, 0.08090579),\n",
       " (191, -0.0909687),\n",
       " (192, -0.08090579),\n",
       " (193, -0.030389985),\n",
       " (194, 0.027371112),\n",
       " (195, -0.055144742),\n",
       " (196, 0.07567307),\n",
       " (197, -0.08251585),\n",
       " (198, 0.08171082),\n",
       " (199, 0.020729592),\n",
       " (200, 0.012327064),\n",
       " (201, 0.007295609),\n",
       " (202, -0.079698235),\n",
       " (203, -0.039647862),\n",
       " (204, -0.042264216),\n",
       " (205, 0.087748565),\n",
       " (206, -0.012327064),\n",
       " (207, 0.059169903),\n",
       " (208, 0.016905688),\n",
       " (209, -0.08734605),\n",
       " (210, 0.085735984),\n",
       " (211, 0.056754805),\n",
       " (212, -0.008100642),\n",
       " (213, 0.006163532),\n",
       " (214, -0.040251635),\n",
       " (215, 0.10022657),\n",
       " (216, 0.03300634),\n",
       " (217, 0.03401263),\n",
       " (218, 0.040654153),\n",
       " (219, 0.023446579),\n",
       " (220, 0.017509462),\n",
       " (221, 0.028377403),\n",
       " (222, -0.014591218),\n",
       " (223, -0.031396277),\n",
       " (224, -6.643092e-05),\n",
       " (225, -0.03099376),\n",
       " (226, 0.09257876),\n",
       " (227, 0.062390037),\n",
       " (228, 0.041459184),\n",
       " (229, 0.04528309),\n",
       " (230, 0.009811336),\n",
       " (231, 0.03642773),\n",
       " (232, 0.02545916),\n",
       " (233, -0.04447806),\n",
       " (234, 0.025660418),\n",
       " (235, 0.0014150966),\n",
       " (236, 0.0332076),\n",
       " (237, 0.04407554),\n",
       " (238, 0.036628988),\n",
       " (239, -0.097811475),\n",
       " (240, 0.026767338),\n",
       " (241, 0.0042012646),\n",
       " (242, -0.03642773),\n",
       " (243, -0.13685556),\n",
       " (244, 0.0019245313),\n",
       " (245, 0.087748565),\n",
       " (246, 0.03542144),\n",
       " (247, 0.008402529),\n",
       " (248, 0.018817639),\n",
       " (249, 0.061182488),\n",
       " (250, 0.04870448),\n",
       " (251, 0.022943432),\n",
       " (252, -0.071245395),\n",
       " (253, -0.051522095),\n",
       " (254, -0.010465425),\n",
       " (255, -0.13122033),\n",
       " (256, 0.012830209),\n",
       " (257, 0.03542144),\n",
       " (258, -0.017308204),\n",
       " (259, 0.06440262),\n",
       " (260, 0.0664152),\n",
       " (261, 0.08171082),\n",
       " (262, -0.041257925),\n",
       " (263, 0.0837234),\n",
       " (264, 0.053132158),\n",
       " (265, 0.09620141),\n",
       " (266, -0.110289484),\n",
       " (267, 0.022742175),\n",
       " (268, 0.020125818),\n",
       " (269, 0.02978621),\n",
       " (270, 0.10062909),\n",
       " (271, 0.10062909),\n",
       " (272, 0.023446579),\n",
       " (273, -0.022540916),\n",
       " (274, -0.013886814),\n",
       " (275, -0.04971077),\n",
       " (276, 0.01438996),\n",
       " (277, -0.05997494),\n",
       " (278, 0.036025215),\n",
       " (279, -0.03421389),\n",
       " (280, 0.0070943506),\n",
       " (281, 0.059169903),\n",
       " (282, -0.050515804),\n",
       " (283, 0.07406301),\n",
       " (284, 0.053937193),\n",
       " (285, -0.06802526),\n",
       " (286, 0.00525787),\n",
       " (287, 0.06601268),\n",
       " (288, -0.012830209),\n",
       " (289, 0.033811375),\n",
       " (290, 0.07164791),\n",
       " (291, -0.069232814),\n",
       " (292, 0.053937193),\n",
       " (293, 0.03522018),\n",
       " (294, -0.08694353),\n",
       " (295, -0.053937193),\n",
       " (296, -0.011924547),\n",
       " (297, -0.043471765),\n",
       " (298, 0.034817666),\n",
       " (299, -0.058364872)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pretrained = W2VPretrainedRetrievalModel(doc_repr_2)\n",
    "w2v_pretrained.train_model()\n",
    "\n",
    "# you can now get an W2V vector for a given query in the following way:\n",
    "w2v_pretrained.vectorize_query(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "65273a2b291a4fbd83e1c4e49b69c6ad",
      "e303d1549f674c03905a4cc1dd8d5de9",
      "d9e360a58de04912b700c6d3db9eb043"
     ]
    },
    "id": "bdprFw4fNyaC",
    "outputId": "27ed4ae1-3caf-4e5a-deaa-5187ccfbb8a1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2f3734f01c416d9fb8ea99f3074ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drm_w2v = DenseRetrievalRanker(w2v, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_w2v.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "a479afdff19e4f35be231aa9a3bc736c",
      "dd1b24f4e41d42b3890c0612460caeb8",
      "6f4ba93864434d0c84f6c3852f416564"
     ]
    },
    "id": "JFPwAh2kNyaC",
    "outputId": "bd1a4f02-9729-4fb7-f678-369678cfb5a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-07a28d4b976b>:31: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if wrd in self.model.wv.vocab:\n",
      "<ipython-input-88-07a28d4b976b>:32: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word_array = self.model.wv[wrd]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a11a939cc443385a211c662e54b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drm_w2v_pretrained = DenseRetrievalRanker(w2v_pretrained, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_w2v_pretrained.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_qt3Md4NyaC",
    "outputId": "9e6a8f6f-b95d-409c-edae-c14f05d1168c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:56:52,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-19 22:56:52,602 : INFO : built Dictionary(5937 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...) from 3204 documents (total 115969 corpus positions)\n",
      "2021-02-19 22:56:52,607 : INFO : discarding 4740 tokens: [('repeat', 8), ('glossari', 7), ('inspect', 8), ('uncol', 2), ('rung', 9), ('secant', 2), ('.', 1603), ('acceler', 6), ('diverg', 3), ('induc', 9)]...\n",
      "2021-02-19 22:56:52,608 : INFO : keeping 1197 tokens which were in no less than 10 and no more than 1602 (=50.0%) documents\n",
      "2021-02-19 22:56:52,610 : INFO : resulting dictionary: Dictionary(1197 unique tokens: ['-', 'algebra', 'intern', 'languag', 'preliminari']...)\n",
      "/Users/xinyichen/opt/anaconda3/lib/python3.8/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2021-02-19 22:56:52,677 : INFO : collecting all words and their counts\n",
      "2021-02-19 22:56:52,678 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-02-19 22:56:52,698 : INFO : collected 5937 word types and 3204 unique tags from a corpus of 3204 examples and 115969 words\n",
      "2021-02-19 22:56:52,699 : INFO : Loading a fresh vocabulary\n",
      "2021-02-19 22:56:52,708 : INFO : effective_min_count=1 retains 5937 unique words (100% of original 5937, drops 0)\n",
      "2021-02-19 22:56:52,708 : INFO : effective_min_count=1 leaves 115969 word corpus (100% of original 115969, drops 0)\n",
      "2021-02-19 22:56:52,723 : INFO : deleting the raw counts dictionary of 5937 items\n",
      "2021-02-19 22:56:52,724 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2021-02-19 22:56:52,725 : INFO : downsampling leaves estimated 92346 word corpus (79.6% of prior 115969)\n",
      "2021-02-19 22:56:52,734 : INFO : estimated required memory for 5937 words and 100 dimensions: 8999700 bytes\n",
      "2021-02-19 22:56:52,735 : INFO : resetting layer weights\n",
      "2021-02-19 22:56:54,301 : INFO : training model with 3 workers on 5937 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-19 22:56:54,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:54,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:54,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:54,481 : INFO : EPOCH - 1 : training on 115969 raw words (95505 effective words) took 0.2s, 546993 effective words/s\n",
      "2021-02-19 22:56:54,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:54,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:54,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:54,646 : INFO : EPOCH - 2 : training on 115969 raw words (95465 effective words) took 0.2s, 591038 effective words/s\n",
      "2021-02-19 22:56:54,816 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:54,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:54,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:54,831 : INFO : EPOCH - 3 : training on 115969 raw words (95677 effective words) took 0.2s, 528865 effective words/s\n",
      "2021-02-19 22:56:54,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,006 : INFO : EPOCH - 4 : training on 115969 raw words (95405 effective words) took 0.2s, 555420 effective words/s\n",
      "2021-02-19 22:56:55,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,179 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,179 : INFO : EPOCH - 5 : training on 115969 raw words (95473 effective words) took 0.2s, 559584 effective words/s\n",
      "2021-02-19 22:56:55,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,347 : INFO : EPOCH - 6 : training on 115969 raw words (95662 effective words) took 0.2s, 584550 effective words/s\n",
      "2021-02-19 22:56:55,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,510 : INFO : EPOCH - 7 : training on 115969 raw words (95453 effective words) took 0.2s, 598953 effective words/s\n",
      "2021-02-19 22:56:55,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,684 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,684 : INFO : EPOCH - 8 : training on 115969 raw words (95567 effective words) took 0.2s, 558282 effective words/s\n",
      "2021-02-19 22:56:55,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:55,845 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:55,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:55,848 : INFO : EPOCH - 9 : training on 115969 raw words (95497 effective words) took 0.2s, 592457 effective words/s\n",
      "2021-02-19 22:56:56,000 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,012 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,012 : INFO : EPOCH - 10 : training on 115969 raw words (95520 effective words) took 0.2s, 594315 effective words/s\n",
      "2021-02-19 22:56:56,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,184 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,185 : INFO : EPOCH - 11 : training on 115969 raw words (95554 effective words) took 0.2s, 569569 effective words/s\n",
      "2021-02-19 22:56:56,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,351 : INFO : EPOCH - 12 : training on 115969 raw words (95495 effective words) took 0.2s, 585019 effective words/s\n",
      "2021-02-19 22:56:56,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,520 : INFO : EPOCH - 13 : training on 115969 raw words (95536 effective words) took 0.2s, 574716 effective words/s\n",
      "2021-02-19 22:56:56,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,683 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,688 : INFO : EPOCH - 14 : training on 115969 raw words (95522 effective words) took 0.2s, 586217 effective words/s\n",
      "2021-02-19 22:56:56,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:56,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:56,860 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:56,861 : INFO : EPOCH - 15 : training on 115969 raw words (95722 effective words) took 0.2s, 563104 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:56:57,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:57,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:57,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:57,034 : INFO : EPOCH - 16 : training on 115969 raw words (95663 effective words) took 0.2s, 566450 effective words/s\n",
      "2021-02-19 22:56:57,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:57,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:57,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:57,204 : INFO : EPOCH - 17 : training on 115969 raw words (95559 effective words) took 0.2s, 577754 effective words/s\n",
      "2021-02-19 22:56:57,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:57,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:57,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:57,371 : INFO : EPOCH - 18 : training on 115969 raw words (95516 effective words) took 0.2s, 580834 effective words/s\n",
      "2021-02-19 22:56:57,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:57,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:57,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:57,530 : INFO : EPOCH - 19 : training on 115969 raw words (95411 effective words) took 0.2s, 613560 effective words/s\n",
      "2021-02-19 22:56:57,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-19 22:56:57,696 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-19 22:56:57,702 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-19 22:56:57,702 : INFO : EPOCH - 20 : training on 115969 raw words (95596 effective words) took 0.2s, 567265 effective words/s\n",
      "2021-02-19 22:56:57,703 : INFO : training on a 2319380 raw words (1910798 effective words) took 3.4s, 561961 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, -0.0019213697),\n",
       " (1, -0.004106078),\n",
       " (2, -0.0028646353),\n",
       " (3, 0.001140005),\n",
       " (4, 0.002845204),\n",
       " (5, 0.00082793355),\n",
       " (6, 0.0022050699),\n",
       " (7, -3.3529705e-06),\n",
       " (8, 0.0022612617),\n",
       " (9, 0.0025844753),\n",
       " (10, 0.0008105231),\n",
       " (11, -0.0036163835),\n",
       " (12, -0.00439776),\n",
       " (13, -0.0006054907),\n",
       " (14, 0.0013738474),\n",
       " (15, 0.0017882313),\n",
       " (16, -0.00018534577),\n",
       " (17, 0.00020566402),\n",
       " (18, 0.00022947349),\n",
       " (19, 0.00036388924),\n",
       " (20, 0.004048445),\n",
       " (21, -0.003983369),\n",
       " (22, -0.0025260183),\n",
       " (23, -0.003400089),\n",
       " (24, 0.0041524353),\n",
       " (25, 0.00031268262),\n",
       " (26, -0.00062451634),\n",
       " (27, -0.0020408237),\n",
       " (28, -0.0015949851),\n",
       " (29, -0.00014304624),\n",
       " (30, 0.0044405507),\n",
       " (31, 0.0026443126),\n",
       " (32, 0.0022147764),\n",
       " (33, 0.003634851),\n",
       " (34, -0.0021013443),\n",
       " (35, -0.004437364),\n",
       " (36, 0.0044091693),\n",
       " (37, 0.0008734586),\n",
       " (38, -0.00043053512),\n",
       " (39, -0.0041146106),\n",
       " (40, 0.0028440086),\n",
       " (41, -0.00081371865),\n",
       " (42, 0.0035586727),\n",
       " (43, 0.0012667307),\n",
       " (44, 0.0019917127),\n",
       " (45, -0.0035935107),\n",
       " (46, 0.00291651),\n",
       " (47, 0.0029622638),\n",
       " (48, 0.001566332),\n",
       " (49, 0.001625369),\n",
       " (50, -0.003622357),\n",
       " (51, 0.002748902),\n",
       " (52, 0.00039842603),\n",
       " (53, -0.0039832583),\n",
       " (54, -0.004601629),\n",
       " (55, 7.1177106e-05),\n",
       " (56, -0.00015762549),\n",
       " (57, 0.004893059),\n",
       " (58, -0.00031336723),\n",
       " (59, 0.0035510464),\n",
       " (60, -0.0023444581),\n",
       " (61, 0.003719291),\n",
       " (62, -0.0022821499),\n",
       " (63, 0.0037675945),\n",
       " (64, -0.0017015286),\n",
       " (65, 0.0039091944),\n",
       " (66, 0.0046770656),\n",
       " (67, -0.0045547485),\n",
       " (68, 0.00034942085),\n",
       " (69, -0.000718187),\n",
       " (70, 0.0034723585),\n",
       " (71, 0.0011964638),\n",
       " (72, 0.0019776053),\n",
       " (73, -0.0012775793),\n",
       " (74, 0.0035043305),\n",
       " (75, -0.0036986067),\n",
       " (76, 0.0025552409),\n",
       " (77, -0.0022056138),\n",
       " (78, -2.467255e-05),\n",
       " (79, 0.004421894),\n",
       " (80, -0.0016948623),\n",
       " (81, -0.0037840146),\n",
       " (82, -0.0018899948),\n",
       " (83, 0.004303288),\n",
       " (84, 0.0009899999),\n",
       " (85, 0.004775341),\n",
       " (86, -0.0022079942),\n",
       " (87, -0.003934818),\n",
       " (88, 0.002237224),\n",
       " (89, 0.002291296),\n",
       " (90, 0.001483248),\n",
       " (91, 0.00033380583),\n",
       " (92, 0.0008041279),\n",
       " (93, -0.0045283763),\n",
       " (94, 0.00047244216),\n",
       " (95, -0.0042049065),\n",
       " (96, 0.0047380077),\n",
       " (97, -0.004254906),\n",
       " (98, -0.0040891245),\n",
       " (99, -0.0008224773)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class D2VRetrievalModel(VectorSpaceRetrievalModel):\n",
    "    def __init__(self, doc_repr):\n",
    "        super().__init__(doc_repr)\n",
    "        \n",
    "        self.vector_size= 100\n",
    "        self.min_count = 1\n",
    "        self.epochs = 20\n",
    "        \n",
    "        self.taggedDocument = [TaggedDocument(doc, [i]) for i, doc in enumerate(self.documents)]\n",
    "        \n",
    "    def train_model(self):\n",
    "        self.model = Doc2Vec(self.taggedDocument, size=self.vector_size, min_count = self.min_count, epochs=self.epochs)\n",
    "    \n",
    "    def vectorize_documents(self):\n",
    "        \"\"\"\n",
    "            Returns a doc_id -> vector dictionary\n",
    "        \"\"\"\n",
    "        vectors = {}\n",
    "\n",
    "        for (doc_id, _), cc in zip(self.doc_repr, self.taggedDocument):\n",
    "          list1 = self.model.infer_vector(cc[0]) # infer vector for the query\n",
    "          list2 = list(range(self.vector_size))\n",
    "\n",
    "          vectors[doc_id] = list(zip(list2, list1))\n",
    "        return vectors\n",
    "\n",
    "\n",
    "    def vectorize_query(self, query):\n",
    "        query = process_text(query, **config_2)\n",
    "\n",
    "        list1 = self.model.infer_vector(query) # infer vector for the query\n",
    "        list2 = list(range(self.vector_size))\n",
    "        \n",
    "        return list(zip(list2, list1))\n",
    "\n",
    "d2v = D2VRetrievalModel(doc_repr_2)\n",
    "d2v.train_model()\n",
    "\n",
    "\n",
    "d2v.vectorize_query(\"mellifluous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9d77db91787642c1885883b271e87e8e",
      "1dc9aeec9c4e41bcb352d45e945489cc",
      "341aa53de823428d86fcb23b8fcfa9a9"
     ]
    },
    "id": "gRmXhn_dNyaC",
    "outputId": "61267b6b-2606-45ee-8e60-b4648cf1973a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463f03f20a414106aa29a49316af8f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drm_d2v = DenseRetrievalRanker(d2v, cosine_sim)\n",
    "\n",
    "# test your LDA model\n",
    "search_fn = drm_d2v.search\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "\n",
    "text.on_submit(handle_submit_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "zsg8N6uyNyaD"
   },
   "outputs": [],
   "source": [
    "class DenseRerankingModel:\n",
    "    def __init__(self, initial_retrieval_fn, vsrm, similarity_fn):\n",
    "        \"\"\"\n",
    "            initial_retrieval_fn: takes in a query and returns a list of [(doc_id, score)] (sorted)\n",
    "            vsrm: instance of `VectorSpaceRetrievalModel`\n",
    "            similarity_fn: function instance that takes in two vectors \n",
    "                            and returns a similarity score e.g cosine_sim defined earlier\n",
    "        \"\"\"\n",
    "        self.ret = initial_retrieval_fn\n",
    "        self.vsrm = vsrm\n",
    "        self.similarity_fn = similarity_fn\n",
    "        self.vectorized_documents = vsrm.vectorize_documents()\n",
    "        \n",
    "        assert len(self.vectorized_documents) == len(doc_repr_2)\n",
    "    \n",
    "    def search(self, query, K=50):\n",
    "        \"\"\"\n",
    "            First, retrieve the top K results using the retrieval function\n",
    "            Then, re-rank the results using the VSRM instance\n",
    "        \"\"\"\n",
    "\n",
    "        # use BM25 to retrieve the top K results and store in newdict\n",
    "        scores = self.ret(query)\n",
    "        doc_ids = [i[0] for i in scores][0:K]\n",
    "        newdict = {k: self.vectorized_documents[k] for k in doc_ids}\n",
    "\n",
    "\n",
    "        empty_list  = []\n",
    "        query_vector = self.vsrm.vectorize_query(query)\n",
    "        \n",
    "        for key in newdict:\n",
    "          doc_id = key\n",
    "          document_single = newdict.get(key)\n",
    "\n",
    "          if document_single !=[] and query_vector != []:\n",
    "            score = self.similarity_fn(document_single, query_vector) # compute similary between the query vector to each vectorized document\n",
    "            empty_list.append(tuple((doc_id, score)))\n",
    "          else:\n",
    "            score = 0\n",
    "            empty_list.append(tuple((doc_id, score)))\n",
    "\n",
    "        empty_list.sort(key=lambda _:-_[1])\n",
    "        return empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUBHrJBCNyaD",
    "outputId": "f6008100-7569-4299-fd2c-185892b4e234"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-07a28d4b976b>:31: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if wrd in self.model.wv.vocab:\n",
      "<ipython-input-88-07a28d4b976b>:32: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word_array = self.model.wv[wrd]\n"
     ]
    }
   ],
   "source": [
    "##### Function check\n",
    "bm25_search_2 = partial(bm25_search, index_set=2)\n",
    "lsi_rerank = DenseRerankingModel(bm25_search_2, lsi, cosine_sim)\n",
    "lda_rerank = DenseRerankingModel(bm25_search_2, lda, jenson_shannon_sim)\n",
    "w2v_rerank = DenseRerankingModel(bm25_search_2, w2v, cosine_sim)\n",
    "w2v_pretrained_rerank = DenseRerankingModel(bm25_search_2, w2v_pretrained, cosine_sim)\n",
    "d2v_rerank = DenseRerankingModel(bm25_search_2, d2v, cosine_sim)\n",
    "\n",
    "##### "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "12341234_12341234_12341234.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01458fb095424240baee28c6b347555c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_33dc764cdf144f79bd8d3ef202ff79bd",
      "placeholder": "​",
      "style": "IPY_MODEL_ac7ec29c04f64e2f962476650c3c4e68",
      "value": ""
     }
    },
    "1dc9aeec9c4e41bcb352d45e945489cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33dc764cdf144f79bd8d3ef202ff79bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "341aa53de823428d86fcb23b8fcfa9a9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ec6170bbcae48dc8c41adba6f85f5da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56084253838740e2b8c958a439e997c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4ec6170bbcae48dc8c41adba6f85f5da",
      "placeholder": "​",
      "style": "IPY_MODEL_5a95c4a6dbe24f91b2c5b7a65376b590",
      "value": ""
     }
    },
    "5a95c4a6dbe24f91b2c5b7a65376b590": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ba4d8180cf1497d8978117ac7322416": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_73825775b25443b0bcab415fd3fcf5aa",
      "placeholder": "​",
      "style": "IPY_MODEL_e9537ba0a1184f7882569dd56d62558a",
      "value": ""
     }
    },
    "65273a2b291a4fbd83e1c4e49b69c6ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_d9e360a58de04912b700c6d3db9eb043",
      "placeholder": "​",
      "style": "IPY_MODEL_e303d1549f674c03905a4cc1dd8d5de9",
      "value": ""
     }
    },
    "6f4ba93864434d0c84f6c3852f416564": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73825775b25443b0bcab415fd3fcf5aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d77db91787642c1885883b271e87e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_341aa53de823428d86fcb23b8fcfa9a9",
      "placeholder": "​",
      "style": "IPY_MODEL_1dc9aeec9c4e41bcb352d45e945489cc",
      "value": ""
     }
    },
    "a479afdff19e4f35be231aa9a3bc736c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Search Bar",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6f4ba93864434d0c84f6c3852f416564",
      "placeholder": "​",
      "style": "IPY_MODEL_dd1b24f4e41d42b3890c0612460caeb8",
      "value": ""
     }
    },
    "ac7ec29c04f64e2f962476650c3c4e68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9e360a58de04912b700c6d3db9eb043": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd1b24f4e41d42b3890c0612460caeb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e303d1549f674c03905a4cc1dd8d5de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9537ba0a1184f7882569dd56d62558a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
